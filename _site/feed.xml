<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://www.sscit.de/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.sscit.de/" rel="alternate" type="text/html" /><updated>2021-02-13T20:24:55+01:00</updated><id>https://www.sscit.de/feed.xml</id><title type="html">Technology and Future.</title><subtitle>Blog about software development and technological progress.</subtitle><author><name>Stefan Schlichthärle</name></author><entry><title type="html">Optimizing REL Part 1 - Performance Measurements</title><link href="https://www.sscit.de/2021/02/03/cpp-performance-measurements.html" rel="alternate" type="text/html" title="Optimizing REL Part 1 - Performance Measurements" /><published>2021-02-03T18:05:31+01:00</published><updated>2021-02-03T18:05:31+01:00</updated><id>https://www.sscit.de/2021/02/03/cpp-performance-measurements</id><content type="html" xml:base="https://www.sscit.de/2021/02/03/cpp-performance-measurements.html">&lt;p&gt;In the last couple of weeks, I have been focusing on optimizing REL’s C++ implementation, mainly for runtime performance and RAM consumption. In a series of blog posts, I will share my approach on how to measure performance metrics for C++ applications running on Linux, derive conclusions and optimize the code afterwards. The first article talks about how to measure performance metrics in Linux, which tools are available and how to generate relevant data.&lt;/p&gt;

&lt;h2 id=&quot;performance-kpis&quot;&gt;Performance KPIs&lt;/h2&gt;

&lt;p&gt;Before optimizing an application, it is important to define performance metrics and collect data about current resource consumption. The data helps to identify hotspots, where lots of resources are consumed, and to compare the effect of different measures. Without measurements and data collection, all optimization efforts maybe address aspects that do not make an impact on the overall performance of the software, or make it even worse.&lt;/p&gt;

&lt;p&gt;Useful metrics to collect are for example runtime of the software while processing a realistic data set, or RAM consumption during the same scenario. Depending on your target system, size of the binary or usage of network or other interfaces are maybe relevant KPIs as well. To optimize REL’s C++ implementation, I have been focusing on runtime and RAM consumption.&lt;/p&gt;

&lt;p&gt;Before starting any measurements, it is also important to define a realistic data set. Measurements collected during “idle mode” of the software or in case of REL, with a tiny dataset, won’t provide meaningful insights. Therefore before pursuing measurements, I defined &lt;a href=&quot;https://github.com/sscit/rel/tree/main/test&quot;&gt;test data sets&lt;/a&gt; of different sizes, to ensure that the system gets under load while processing the data.&lt;/p&gt;

&lt;h2 id=&quot;measuring-runtime&quot;&gt;Measuring Runtime&lt;/h2&gt;

&lt;p&gt;Linux Tool &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;time&lt;/code&gt; provides an easy way to measure the runtime of a program. Just run it with the program and its parameters:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;time bazel-bin/rel-cli/rel_cli -r ./test/big 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It then returns three values, namely&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;real    0m2.509s
user    0m2.461s
sys	    0m0.032s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;real&lt;/code&gt; describes the time elapsed from start to finish of the program, the so called wall clock time. It includes waiting times and time, where the process was not scheduled at all.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user&lt;/code&gt; is the time that was actually spent within the user space process, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sys&lt;/code&gt; shows the time spent in the kernel within the process.&lt;/p&gt;

&lt;p&gt;If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;time&lt;/code&gt; is run multiple times and with different software versions, a data set can be built up and the effect of changes can be evaluated.&lt;/p&gt;

&lt;h3 id=&quot;identify-expensive-code&quot;&gt;Identify Expensive Code&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;time&lt;/code&gt; gives a first indication about the overall runtime, but it does not provide any details about which parts of the code take a long time. To get this kind of information, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf&lt;/code&gt; can be used. It executes the process, similar to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;time&lt;/code&gt;, but collects data during execution, which can be used to generate a report afterwards.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo perf record &amp;lt;binary&amp;gt; &amp;lt;binary_options&amp;gt;
sudo perf report
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The report lists functions sorted by their CPU consumption during the execution.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Samples: 2K of event 'cpu-clock:pppH', Event count (approx.): 740750000
Overhead  Command  Shared Object        Symbol
  15.69%  rel_cli  libc-2.31.so         [.] __memcmp_avx2_movbe
  12.55%  rel_cli  rel_cli              [.] std::_Rb_tree&amp;lt;std::__cxx11::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, std::allocator&amp;lt;
   8.64%  rel_cli  rel_cli              [.] Lexer::Read
   4.86%  rel_cli  libstdc++.so.6.0.28  [.] std::istream::get
   3.27%  rel_cli  rel_cli              [.] Lexer::IsOperatorOrKeyword
   3.24%  rel_cli  libstdc++.so.6.0.28  [.] std::__cxx11::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, std::allocator&amp;lt;char&amp;gt; &amp;gt;::compa
   3.17%  rel_cli  libstdc++.so.6.0.28  [.] std::istream::sentry::sentry
   3.10%  rel_cli  libstdc++.so.6.0.28  [.] std::__cxx11::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, std::allocator&amp;lt;char&amp;gt; &amp;gt;::_M_re
   2.73%  rel_cli  libc-2.31.so         [.] __memmove_avx_unaligned_erms
   2.40%  rel_cli  libc-2.31.so         [.] __memmove_avx_unaligned
   2.19%  rel_cli  libc-2.31.so         [.] malloc
   2.16%  rel_cli  libc-2.31.so         [.] _int_malloc
   2.13%  rel_cli  rel_cli              [.] FileReader::GetChar
   1.89%  rel_cli  rel_cli              [.] Lexer::IsOperator
   1.86%  rel_cli  libstdc++.so.6.0.28  [.] std::__cxx11::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, std::allocator&amp;lt;char&amp;gt; &amp;gt;::_M_re
   1.65%  rel_cli  libc-2.31.so         [.] __strlen_avx2
   1.52%  rel_cli  rel_cli              [.] Lexer::IsLinebreak
   1.38%  rel_cli  libstdc++.so.6.0.28  [.] operator new
   1.21%  rel_cli  rel_cli              [.] Lexer::CheckStringandAddToken
   1.18%  rel_cli  rel_cli              [.] SlidingWindow::pop_front
   1.15%  rel_cli  libc-2.31.so         [.] _int_free
   1.15%  rel_cli  libstdc++.so.6.0.28  [.] std::__cxx11::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, std::allocator&amp;lt;char&amp;gt; &amp;gt;::_M_ap
   1.15%  rel_cli  rel_cli              [.] Lexer::AddTokenToList
   1.15%  rel_cli  rel_cli              [.] RdParser::ReadString
   1.11%  rel_cli  libc-2.31.so         [.] cfree@GLIBC_2.2.5
   1.11%  rel_cli  libc-2.31.so         [.] isalnum
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this excerpt, we can see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rel_cli&lt;/code&gt; application spends most of its time on memcopy operations, due to building up AST and token lists containing lots of strings. Additionally, method &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Read()&lt;/code&gt; within class &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Lexer&lt;/code&gt; is worth investigating. A report like this is very useful to focus on the most expensive methods first, that may offer most potential for optimizations, too. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf&lt;/code&gt; tool is pretty straightforward to use, too, and doesn’t require special compilation flags. The release binary including optimized code can be used for analysis.&lt;/p&gt;

&lt;h2 id=&quot;measuring-ram-consumption&quot;&gt;Measuring RAM consumption&lt;/h2&gt;

&lt;p&gt;Another resource that is worth optimizing is the amount of RAM required during execution. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bin/time/ -v&lt;/code&gt; provides valuable insights on this KPI:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/bin/time -v bazel-bin/rel-cli/rel_cli -r ./test/big 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Besides other information, it returns the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Maximum resident set size (kbytes):&lt;/code&gt;, which indicates the maximum amount of physical RAM allocated by the process during runtime. By running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bin/time/&lt;/code&gt; multiple times with different software versions, the impact of optimization measures in terms of RAM consumption can be evaluated.&lt;/p&gt;

&lt;p&gt;In the next blog post, I will describe how I evaluated REL’s performance with these tools, and describe measures that improved the KPIs over time.&lt;/p&gt;</content><author><name>Stefan Schlichthärle</name></author><summary type="html">In the last couple of weeks, I have been focusing on optimizing REL’s C++ implementation, mainly for runtime performance and RAM consumption. In a series of blog posts, I will share my approach on how to measure performance metrics for C++ applications running on Linux, derive conclusions and optimize the code afterwards. The first article talks about how to measure performance metrics in Linux, which tools are available and how to generate relevant data.</summary></entry><entry><title type="html">Optimizing REL Part 1 - Optimizing the Code</title><link href="https://www.sscit.de/2021/02/03/rel-optimization.html" rel="alternate" type="text/html" title="Optimizing REL Part 1 - Optimizing the Code" /><published>2021-02-03T18:05:31+01:00</published><updated>2021-02-03T18:05:31+01:00</updated><id>https://www.sscit.de/2021/02/03/rel-optimization</id><content type="html" xml:base="https://www.sscit.de/2021/02/03/rel-optimization.html">&lt;p&gt;In the last couple of weeks, I have been focusing on optimizing REL’s C++ implementation, mainly for runtime performance and RAM consumption. In a series of blog posts, I will share my approach on how to measure performance metrics for C++ applications running on Linux, derive conclusions and optimize the code afterwards. This blog post will focus on C++ optimizations I applied to the code.&lt;/p&gt;

&lt;h2 id=&quot;baseline-and-test-data-set&quot;&gt;Baseline and Test Data Set&lt;/h2&gt;

&lt;p&gt;In the [last blog post]/2021/02/03/cpp-performance-measurements.html, I wrote about different tools and approaches, how to measure performance of C++ applications in Linux. I applied them to my implementation of REL, namely the core library and the surrounding CLI wrapper. To get representative values, I defined the &lt;a href=&quot;https://github.com/sscit/rel/tree/main/test/huge&quot;&gt;huge&lt;/a&gt; test data set, which consists of more than 400k requirements, distributed among 225 files, which sum up to around 500 MB. The amount of requirements is way more than even in large software projects, but the specification of this test project is rather simple. Nevertheless, enough food for the software, to spot the impact of different implementations in its measurements.&lt;/p&gt;

&lt;p&gt;Baseline was &lt;a href=&quot;https://github.com/sscit/rel/commit/9fa79fd58bc29eae549abc89e8b48fd4e01bd562&quot;&gt;9fa79fd&lt;/a&gt; and the following values:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Wall Time: 5min 14s
RAM Consumption: 5,7 GB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the wall time obviously depends a lot on the machine used to measure. As I am currently developing in an Ubuntu VM, the absolute value is not representative at all, but the change over time is for sure. Anyway, the baseline doesn’t look like performant software, 5 minutes for 500 MB of data, and eating up more than 5GB of RAM. These values clearly reflect the state of the software at this point in time, I didn’t apply any optimization, I rather focused on features and clean implementation.&lt;/p&gt;

&lt;h2 id=&quot;optimizing-the-lexer&quot;&gt;Optimizing the Lexer&lt;/h2&gt;

&lt;p&gt;After running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf&lt;/code&gt; on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rel_cli&lt;/code&gt; and collecting first measurements, I quickly identified a couple of slow methods within class &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Lexer&lt;/code&gt;: They were called a lot during lexing of the files and consumed significant runtime, due to their poor implementation (see [05915cb])(https://github.com/sscit/rel/commit/05915cb4982165e6e19dd2cbccdb7433422db01b). Elimination of copies and using more tailored data structures already made an impact here.&lt;/p&gt;

&lt;p&gt;Additionally, I reworked the overall data flow of the library (see &lt;a href=&quot;https://github.com/sscit/rel/commit/1bdb698fb4c486a1c2471a8223a7f130a6358fad&quot;&gt;1bdb698&lt;/a&gt;). Before, all files were lexed first, and all tokens were stored in memory, which is not necessary at all. An improved workflow goes like this: Reading one file first, creating the tokens, and parsing them afterwards immediately. In this case, the tokens created can be deleted, before starting with the next file. This change alone had a significant impact on RAM consumption.&lt;/p&gt;

&lt;p&gt;After the first iteration of optimizing the code, the new measurements looked like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Wall Time: 2min 47s
RAM Consumption: 2,7 GB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;50% off! Sounds like a good start. There are always low-hanging fruits that can be grabbed easily.&lt;/p&gt;

&lt;h2 id=&quot;eliminating-redundant-copying-of-data&quot;&gt;Eliminating Redundant Copying of Data&lt;/h2&gt;

&lt;p&gt;During the next iteration, I had a look on duplicate data storage within the key data classes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt; and the ones used in the AST. For example, in all objects of class &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Token&lt;/code&gt;, I stored a string containing the full filename, where the token originated from. Same applied for objects of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RdTypeInstance&lt;/code&gt;, which represent a single type instance from the requirements data. For projects consisting of more than 400k requirements, 400k times the size of a string containing a filename indeed makes an impact. I replaced all those occurrences with a pointer to the filename. This optimization, and removing other redundant data, further decreased RAM consumption to less than 1 GB (see &lt;a href=&quot;https://github.com/sscit/rel/commit/921705df19a99372b9742245bfa8801092a65e61&quot;&gt;921705d&lt;/a&gt;). Impact on runtime was only minor though.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Wall Time: 2min 40s
RAM Consumption: 800 MB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;using-multiple-threads-to-read-and-parse-requirements-data&quot;&gt;Using Multiple Threads to Read and Parse Requirements Data&lt;/h2&gt;

&lt;p&gt;While testing different configurations and collecting data, I noticed that rel_cli was running on a single core only. Of course, it was a single-threaded application, without any code running in parallel, therefore why should it benefit from all cores available in the system? Therefore I made some experiments to parallelize the library: A thread shall be created for every file, which lexes and parses the file and stores the resulting AST elements within the central data structure (see &lt;a href=&quot;https://github.com/sscit/rel/commit/b18f8075686b7631d000d054d3dfd5f2d45c9d8c&quot;&gt;b18f807&lt;/a&gt;). In C++17, spawning multiple threads is quite straightforward. I defined a function closure, which handles a single file. Additionally, to avoid race conditions, I introduced mutexes in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RdParser&lt;/code&gt;, whenever shared data sources are updated. The results where quite whopping: Wall time went further down to nearly one minute only, another improvement by more than 60% compared to the last iteration. But of course, there is no free lunch: Memory consumption increased again significantly, of course, multiple threads in memory means that N files are processed in parallel and the resulting tokens have to be kept in RAM.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Wall Time: 1min 01s
RAM Consumption: 2,4 GB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In REL’s CI validation on Github, that is fortunately based on way more powerful IT compared to my VM, the performance metrics can be seen, too. There the validation of the huge test project went down from approx. 1 minute runtime to less than &lt;a href=&quot;https://github.com/sscit/rel/runs/1849966084&quot;&gt;20s&lt;/a&gt;. With this result, I consider requirement &lt;a href=&quot;https://github.com/sscit/rel/blob/main/requirements/5_performance.rd#L4&quot;&gt;perf1&lt;/a&gt; as fulfilled!&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;To sum up, optimizing C++ code brings tremendous improvements in terms of resource consumptions. It is definitely worth to spent this effort, because fast software is way more user friendly compared to slow execution times and is a real business value.&lt;/p&gt;

&lt;p&gt;Especially for a tool like REL, which is used by individual users as well as during CI validation, the business value is significant: For the user, optimized code for both RAM and runtime increases productivity and also enables users on “slow” machines, for example virtual machines, to use the tools without frustration. In corporations, where Windows is the dominant operating system, lots of people use Linux via virtual desktop or virtual machine, therefore this environment is a relevant use case. In CI, every second that is saved pays into the overall goal of fast iteration cycles and may helps to reduce cost for the cloud.&lt;/p&gt;

&lt;p&gt;I am still playing around for further optimization. Due to the comprehensive test coverage of REL, introducing further changes imposes a very low risk to break something fundamental. I am still wondering if there is a sweet spot between number of threads, CPU cores available and the corresponding RAM usage. And I would like to try out if mapping the whole file in memory first may introduces another speed gain.&lt;/p&gt;</content><author><name>Stefan Schlichthärle</name></author><summary type="html">In the last couple of weeks, I have been focusing on optimizing REL’s C++ implementation, mainly for runtime performance and RAM consumption. In a series of blog posts, I will share my approach on how to measure performance metrics for C++ applications running on Linux, derive conclusions and optimize the code afterwards. This blog post will focus on C++ optimizations I applied to the code.</summary></entry><entry><title type="html">Unit Testing with Google Test in Bazel</title><link href="https://www.sscit.de/2021/01/08/bazel-gtest.html" rel="alternate" type="text/html" title="Unit Testing with Google Test in Bazel" /><published>2021-01-08T18:05:31+01:00</published><updated>2021-01-08T18:05:31+01:00</updated><id>https://www.sscit.de/2021/01/08/bazel-gtest</id><content type="html" xml:base="https://www.sscit.de/2021/01/08/bazel-gtest.html">&lt;p&gt;&lt;a href=&quot;https://github.com/google/googletest&quot;&gt;Google Test&lt;/a&gt; is a well-established framework for unit tests in C++. It provides lots of features and can be used to write tests for own classes and their methods. Its integration in Bazel build system works quite well, with the benefit that it is not necessary to copy Google Test source files into the own repository or use Git submodules, as Google Test’s repository is downloaded on demand by Bazel during the build process. In this blog post, I will describe how &lt;a href=&quot;/2021/01/03/REL.html&quot;&gt;REL&lt;/a&gt; uses Google Test. This approach can easily be transferred to every C++ development project that uses Bazel as build system.&lt;/p&gt;

&lt;p&gt;First of all, Google Test has to be added as external dependency to the Bazel WORKSPACE file. Bazel’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new_git_repository&lt;/code&gt; rule can be used for this.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;new_git_repository(
    name = &quot;googletest&quot;,
    build_file = &quot;gmock.BUILD&quot;,
    remote = &quot;https://github.com/google/googletest&quot;,
    tag = &quot;release-1.10.0&quot;,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/sscit/rel/blob/main/WORKSPACE#L3&quot;&gt;Source in REL project&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this snippet, Google Test’s public repo on Github is referenced, and a dedicated tag is selected. If there is a new version of Google Test available, it is sufficient to update the tag in this rule, to change the dependency for all unit tests in the workspace.&lt;/p&gt;

&lt;p&gt;As a second step, build file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gmock.BUILD&lt;/code&gt; is created in folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;external&lt;/code&gt;. It contains the Bazel definitions for Google Test, so that the referenced code is available as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_library&lt;/code&gt;’s&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cc_library(
    name = &quot;gtest&quot;,
    srcs = [
        &quot;googletest/src/gtest-all.cc&quot;,
        &quot;googlemock/src/gmock-all.cc&quot;,
    ],
    hdrs = glob([
        &quot;**/*.h&quot;,
        &quot;googletest/src/*.cc&quot;,
        &quot;googlemock/src/*.cc&quot;,
    ]),
    includes = [
        &quot;googlemock&quot;,
        &quot;googletest&quot;,
        &quot;googletest/include&quot;,
        &quot;googlemock/include&quot;,
    ],
    linkopts = [&quot;-pthread&quot;],
    visibility = [&quot;//visibility:public&quot;],
)

cc_library(
    name = &quot;gtest_main&quot;,
    srcs = [&quot;googlemock/src/gmock_main.cc&quot;],
    linkopts = [&quot;-pthread&quot;],
    visibility = [&quot;//visibility:public&quot;],
    deps = [&quot;:gtest&quot;],
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/sscit/rel/blob/main/external/gmock.BUILD&quot;&gt;Source in REL project&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The first library includes all Google Test related source files, whereas the second rule only contains the reference to Google Test’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;Now Google Test is available in the build environment as external dependency. If it is required as part of a build, it is downloaded and built (and cached) by Bazel. To use it for own unittests, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gtest_main&lt;/code&gt; rule is referenced as dependency by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_test&lt;/code&gt; rule, which also includes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*Test.cpp&lt;/code&gt; - files, which contain the test cases.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cc_test(
    name = &quot;RelLibUnitTest&quot;,
    srcs = glob([&quot;**/*.cpp&quot;]),
    deps = [
        &quot;//rel-lib:rel_lib&quot;,
        &quot;@googletest//:gtest_main&quot;,
    ],
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/sscit/rel/blob/main/rel-lib/test/unittest/BUILD&quot;&gt;Source in REL project&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_test&lt;/code&gt; is the “executable”, which brings together the test cases (in this example, added to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;srcs&lt;/code&gt;), the actual source code that shall be tested (dependency to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rel_lib&lt;/code&gt;) and the Google Test Framework, referenced as dependency, too.&lt;/p&gt;

&lt;p&gt;To run it, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bazel test //path/to:RelLibUnitTest&lt;/code&gt; is executed. Google Test logfiles are not printed out to the command line, they are stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/bazel-testlogs/path/to/RelLibUnitTest&lt;/code&gt;.&lt;/p&gt;</content><author><name>Stefan Schlichthärle</name></author><summary type="html">Google Test is a well-established framework for unit tests in C++. It provides lots of features and can be used to write tests for own classes and their methods. Its integration in Bazel build system works quite well, with the benefit that it is not necessary to copy Google Test source files into the own repository or use Git submodules, as Google Test’s repository is downloaded on demand by Bazel during the build process. In this blog post, I will describe how REL uses Google Test. This approach can easily be transferred to every C++ development project that uses Bazel as build system.</summary></entry><entry><title type="html">Building and Testing C++ Python Modules with Bazel</title><link href="https://www.sscit.de/2021/01/05/python-bazel.html" rel="alternate" type="text/html" title="Building and Testing C++ Python Modules with Bazel" /><published>2021-01-05T16:05:31+01:00</published><updated>2021-01-05T16:05:31+01:00</updated><id>https://www.sscit.de/2021/01/05/python-bazel</id><content type="html" xml:base="https://www.sscit.de/2021/01/05/python-bazel.html">&lt;p&gt;While working on &lt;a href=&quot;/2021/01/03/REL.html&quot;&gt;REL&lt;/a&gt;, I learned a lot about Bazel and its usage as build system in open source projects. In a series of blog posts, I will share these learnings and describe different approaches. Today’s blog post addresses the integration of C++-based Python modules into Bazel and the modelling of dependencies towards the corresponding Python-based tests.&lt;/p&gt;

&lt;p&gt;As part of REL’s Python integration, I created a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_library&lt;/code&gt; called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rel_py&lt;/code&gt; which includes the core REL C++ library and the necessary Python binding (using the ingenious &lt;a href=&quot;https://github.com/pybind/pybind11&quot;&gt;pybind11&lt;/a&gt; framework). If the library is built as dynamic library, the resulting &lt;em&gt;librel_py.so&lt;/em&gt; file can directly be imported in every Python script via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import&lt;/code&gt; statement. It took me quite some time, though, to model the dependency between a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py_test&lt;/code&gt; rule and the mentioned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_library&lt;/code&gt;. My goal was to add an integration test to Bazel, which uses REL within Python, to read a toy model and test the basic functionality, like accessing all type instances, checking the API etc.&lt;/p&gt;

&lt;p&gt;The integration test itself is a simple python script, that imports &lt;em&gt;librel_py.so&lt;/em&gt; and interacts with the API. I wrapped it into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py_test&lt;/code&gt; rule. At the moment, it is not possible, though, to model a dependency (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deps&lt;/code&gt;) in Bazel from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py_test&lt;/code&gt; towards &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_library&lt;/code&gt;, as Bazel only allows dependencies towards rules from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py_&lt;/code&gt; family. Therefore I tried the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; - attribute, which allows specifying arbitrary dependencies, e.g. to test data. Unfortunately, with this approach, I was not able to specify the correct import paths for the Python runtime. During test execution, Python always complained, that the module that shall be imported cannot be found.&lt;/p&gt;

&lt;p&gt;After searching on Stackoverflow and the Bazel bugtracker, I finally figured out the following approach, to get the dependencies right: Apparently it is necessary to define a dummy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py_library&lt;/code&gt; first, which is modeled as dependency within the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py_test&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py_library&lt;/code&gt; then uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; attribute to point to a &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_binary&lt;/code&gt;&lt;/strong&gt; rule, which is located in the &lt;strong&gt;same folder&lt;/strong&gt; as the two py-rules, and is actually a copy of the original &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc_library&lt;/code&gt;. The disadvantage of this solution is definitely, that the Bazel model is partially duplicated. Nevertheless, the obvious advantage, it now works and I can run an integration test via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bazel test&lt;/code&gt;, that builds the Python binding library/binary of REL and runs a Python script, to test the functionality.&lt;/p&gt;

&lt;p&gt;My solution in Bazel can be found here: &lt;a href=&quot;https://github.com/sscit/rel/blob/main/relpy/test/BUILD&quot;&gt;https://github.com/sscit/rel/blob/main/relpy/test/BUILD&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Bazel Bugtracker issues related to this topic, that contain additional details:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/bazelbuild/bazel/issues/1475&quot;&gt;https://github.com/bazelbuild/bazel/issues/1475&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/bazelbuild/bazel/issues/701&quot;&gt;https://github.com/bazelbuild/bazel/issues/701&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Stefan Schlichthärle</name></author><summary type="html">While working on REL, I learned a lot about Bazel and its usage as build system in open source projects. In a series of blog posts, I will share these learnings and describe different approaches. Today’s blog post addresses the integration of C++-based Python modules into Bazel and the modelling of dependencies towards the corresponding Python-based tests.</summary></entry><entry><title type="html">REL - Requirements Engineering Language</title><link href="https://www.sscit.de/2021/01/03/REL.html" rel="alternate" type="text/html" title="REL - Requirements Engineering Language" /><published>2021-01-03T21:05:31+01:00</published><updated>2021-01-03T21:05:31+01:00</updated><id>https://www.sscit.de/2021/01/03/REL</id><content type="html" xml:base="https://www.sscit.de/2021/01/03/REL.html">&lt;p&gt;In the last couple of weeks, I have been working on an open source project called &lt;a href=&quot;https://github.com/sscit/rel/blob/main/README.md&quot;&gt;REL - Requirements Engineering Language&lt;/a&gt;. Its core is a domain specific language, which enables requirements engineers to define a so called requirements model. The model contains type definitions and enumerations, which are then used to formally describe the actual “content”, i.e. the requirements written by developers. All data resides in text files, which can be committed into a git repository. Besides the DSL, the REL framework contains a (partially completed) implementation of the language server protocol, to get IDE support for the language, and python integration. For more technical details about the framework, have a look at the &lt;a href=&quot;https://github.com/sscit/rel/blob/main/README.md&quot;&gt;README.md&lt;/a&gt; on Github or the still growing &lt;a href=&quot;https://github.com/sscit/rel/blob/main/doc/developers_guide.md&quot;&gt;developer’s guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Requirements engineering in software projects always consists of three parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The actual content, which means the requirements that describe the resulting product and the metadata like attributes, links, comments to the requirements etc.&lt;/li&gt;
  &lt;li&gt;The tooling to manage the requirements (like DOORS or, yes, as usual, the multi-purpose powertool Excel)&lt;/li&gt;
  &lt;li&gt;The processes, that define how requirements work is done in the project, e.g. collaboration with partners, quality metrics, tracing of requirements towards tests and others.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2 and 3 are ideally covered by the requirements manager(s) of the project, whose role cover establishing processes and tools for requirements work within the project. Processes and tools, well documented and supporting an agile workflow, are the enabler for #1: All project members have to contribute to the “content”, as in most projects, the knowledge about the product is distributed among all team members. Based on my experience, projects often neglect the efforts to establish solid processes and tools for requirements engineering, which are ideally available right at the beginning of the project! If this is not the case, developers don’t know where to put their requirements, and in worst case, gradually loose track on this fundamental part of their (software) development work.&lt;/p&gt;

&lt;p&gt;To address this challenge, I created the REL framework, whose purpose is already well described in the &lt;a href=&quot;https://github.com/sscit/rel/blob/main/doc/developers_guide.md&quot;&gt;developers guide&lt;/a&gt;, therefore I quote it here again:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In a nutshell, the REL framework shall enable requirements engineers to define a requirements model and the corresponding processes, which helps the whole team to focus on the actual work of writing valuable requirements. With the domain specific language as core element, well-known agile processes can be applied. The tooling provided in the framework is focusing on clear usecases, and always provides hooks for project-specific extensions. In an ideal world, requirements engineers use the REL framework to define tooling and processes upfront, so that developers can then focus on the creative work of writing requirements.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Stefan Schlichthärle</name></author><summary type="html">In the last couple of weeks, I have been working on an open source project called REL - Requirements Engineering Language. Its core is a domain specific language, which enables requirements engineers to define a so called requirements model. The model contains type definitions and enumerations, which are then used to formally describe the actual “content”, i.e. the requirements written by developers. All data resides in text files, which can be committed into a git repository. Besides the DSL, the REL framework contains a (partially completed) implementation of the language server protocol, to get IDE support for the language, and python integration. For more technical details about the framework, have a look at the README.md on Github or the still growing developer’s guide.</summary></entry><entry><title type="html">Website Relaunch</title><link href="https://www.sscit.de/2020/12/29/first-post.html" rel="alternate" type="text/html" title="Website Relaunch" /><published>2020-12-29T21:05:31+01:00</published><updated>2020-12-29T21:05:31+01:00</updated><id>https://www.sscit.de/2020/12/29/first-post</id><content type="html" xml:base="https://www.sscit.de/2020/12/29/first-post.html">&lt;p&gt;I finally re-enabled my website sscit.de, after couple of years being disabled. In future, I plan to publish technical articles related to technology, mostly computer science, and maybe some related fields.&lt;/p&gt;</content><author><name>Stefan Schlichthärle</name></author><summary type="html">I finally re-enabled my website sscit.de, after couple of years being disabled. In future, I plan to publish technical articles related to technology, mostly computer science, and maybe some related fields.</summary></entry></feed>